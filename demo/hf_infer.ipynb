{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["OD5ITGgLZb09"],"gpuType":"T4","authorship_tag":"ABX9TyMfTpFL0J6kEknyPwmqX9fx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Run Gemma-2b on Google Colab GPU"],"metadata":{"id":"FXXG6pV6ZUCe"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"OD5ITGgLZb09"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"anhS1vZMT18b","executionInfo":{"status":"ok","timestamp":1709705674629,"user_tz":-330,"elapsed":76901,"user":{"displayName":"Vakyansh","userId":"14121683683494953777"}}},"outputs":[],"source":["%%capture\n","import torch\n","major_version, minor_version = torch.cuda.get_device_capability()\n","if major_version >= 8:\n","    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n","    !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\n","else:\n","    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n","    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n","pass\n","\n","!pip install -q gradio"]},{"cell_type":"code","source":["## Some Imports\n","\n","import accelerate\n","import gradio as gr\n","import torch, os\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import StoppingCriteria, TextIteratorStreamer\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer\n","from threading import Thread"],"metadata":{"id":"mfYDmD9wZJrO","executionInfo":{"status":"ok","timestamp":1709705764470,"user_tz":-330,"elapsed":7889,"user":{"displayName":"Vakyansh","userId":"14121683683494953777"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Set the model you want to use"],"metadata":{"id":"eMoeDq0XZjdc"}},{"cell_type":"code","source":["# Load tokenizer and model from Hugging Face's model hub\n","MODEL_NAME = \"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\"\n","hf_token = \"<your hf token>\""],"metadata":{"id":"GTSkSsi8aEqn","executionInfo":{"status":"ok","timestamp":1709705989462,"user_tz":-330,"elapsed":700,"user":{"displayName":"Vakyansh","userId":"14121683683494953777"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Set the number of threads for Torch\n","torch.set_num_threads(2)\n","\n","\n","# Set device to CUDA if available, otherwise to CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit = False, token=hf_token).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","\n","# Function to count tokens\n","def count_tokens(text):\n","    return len(tokenizer.tokenize(text))"],"metadata":{"id":"1AmbB_37Ze-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run Gradio Interface"],"metadata":{"id":"GJIJP1u4bJMG"}},{"cell_type":"code","source":["# Function to generate model predictions\n","def predict(message, history):\n","    alpaca_format = \"\"\"\n","    ### Instruction:\n","    {}\n","\n","    ### Input:\n","    {}\n","\n","    ### Response:\n","    {}\"\"\"\n","\n","    # if no input is presetn make it work\n","    if '###' not in message:\n","        message = message + ' ### '\n","\n","    # Split message into instruction and input\n","    messages = message.split('###')\n","    model_inputs = tokenizer(alpaca_format.format(messages[0], messages[1], \"\"), return_tensors=\"pt\").to(device)\n","\n","    # Initialize TextIteratorStreamer\n","    streamer = TextIteratorStreamer(tokenizer, timeout=120., skip_prompt=True, skip_special_tokens=True)\n","\n","    # Generate model kwargs\n","    generate_kwargs = dict(\n","        model_inputs,\n","        streamer=streamer,\n","        max_new_tokens=2048 - count_tokens(alpaca_format),\n","        # top_p=0.2,\n","        # top_k=20,\n","        # temperature=0.1,\n","        repetition_penalty=2.0,\n","        # length_penalty=-0.5,\n","        # num_beams=1\n","\n","    )\n","\n","    # Start generation in a separate thread\n","    t = Thread(target=model.generate, kwargs=generate_kwargs)\n","    t.start()\n","\n","    # Yield partial message\n","    partial_message = \"\"\n","    for new_token in streamer:\n","        partial_message += new_token\n","        yield partial_message\n","\n","\n","# Setting up the Gradio chat interface\n","gr.ChatInterface(predict,\n","                 title=\"Navarasa 2b chat demo\",\n","                 description=None\n","                 ).launch(share=True)  # Launching the web interface.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":590},"id":"f4J9vYjRaIjw","executionInfo":{"status":"ok","timestamp":1709706734297,"user_tz":-330,"elapsed":5573,"user":{"displayName":"Vakyansh","userId":"14121683683494953777"}},"outputId":"3b547085-0374-4217-ea6d-bb64fb884961"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://ec81cea79e17ca3a13.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ec81cea79e17ca3a13.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"UEKFrSl4bM9H"},"execution_count":null,"outputs":[]}]}