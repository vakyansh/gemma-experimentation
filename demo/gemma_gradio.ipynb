{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"14uURzlJWFVSlkqluFbL7-Bnp8J-jul7L","authorship_tag":"ABX9TyNPLcHPuJmQ9dMKOEUSde42"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wTrLKQlYMKqI"},"outputs":[],"source":["import accelerate\n","import gradio as gr\n","import torch\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n","from threading import Thread\n","from unsloth import FastLanguageModel\n","\n","\n","# Set the number of threads for Torch\n","torch.set_num_threads(1)\n","\n","# Get Hugging Face token from environment variable\n","HF_TOKEN = '< your hf token >'\n","\n","# Set device to CUDA if available, otherwise to CPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Load tokenizer and model from Hugging Face's model hub\n","MODEL_NAME = \"< your model name >\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN).to(device)\n","FastLanguageModel.for_inference(model)\n","\n","\n","# Function to count tokens\n","def count_tokens(text):\n","    return len(tokenizer.tokenize(text))\n"]},{"cell_type":"code","source":["# Function to generate model predictions\n","def predict(message, history):\n","    alpaca_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","    ### Instruction:\n","    {}\n","\n","    ### Input:\n","    {}\n","\n","    ### Response:\n","    {}\"\"\"\n","\n","    # if no input is presetn make it work\n","    if '###' not in message:\n","        message = message + ' ### '\n","\n","    # Split message into instruction and input\n","    messages = message.split('###')\n","    model_inputs = tokenizer(alpaca_format.format(messages[0], messages[1], \"\"), return_tensors=\"pt\").to(device)\n","\n","    # Initialize TextIteratorStreamer\n","    streamer = TextIteratorStreamer(tokenizer, timeout=120., skip_prompt=True, skip_special_tokens=True)\n","\n","    # Generate model kwargs\n","    generate_kwargs = dict(\n","        model_inputs,\n","        streamer=streamer,\n","        max_new_tokens=2048 - count_tokens(alpaca_format),\n","        top_p=0.2,\n","        top_k=20,\n","        temperature=0.1,\n","        repetition_penalty=2.0,\n","        length_penalty=-0.5,\n","        num_beams=1\n","    )\n","\n","    # Start generation in a separate thread\n","    t = Thread(target=model.generate, kwargs=generate_kwargs)\n","    t.start()\n","\n","    # Yield partial message\n","    partial_message = \"\"\n","    for new_token in streamer:\n","        partial_message += new_token\n","        yield partial_message\n","\n","\n","# Setting up the Gradio chat interface\n","gr.ChatInterface(predict,\n","                 title=\"Gemma 2b Instruct Chat\",\n","                 description=None\n","                 ).launch(share=True)  # Launching the web interface.\n"],"metadata":{"id":"RSg2Y5UCMQev"},"execution_count":null,"outputs":[]}]}